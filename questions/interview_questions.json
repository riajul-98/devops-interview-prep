{
    "questions": [
      {
        "id": "aws-001",
        "topic": "aws",
        "difficulty": "easy",
        "question": "Which AWS service provides object storage?",
        "options": [
          "EC2",
          "S3",
          "RDS",
          "Lambda"
        ],
        "correct_answer": 2,
        "explanation": "Amazon S3 (Simple Storage Service) is AWS's object storage service that offers industry-leading scalability, data availability, security, and performance."
      },
      {
        "id": "aws-002",
        "topic": "aws",
        "difficulty": "medium",
        "question": "What is the best practice for storing sensitive configuration data in AWS?",
        "options": [
          "Store in EC2 user data",
          "Hardcode in application",
          "Use AWS Systems Manager Parameter Store or Secrets Manager",
          "Store in S3 bucket"
        ],
        "correct_answer": 3,
        "explanation": "AWS Systems Manager Parameter Store and Secrets Manager are designed specifically for storing sensitive configuration data securely with encryption and access controls.",
        "scenario": "You're deploying a web application that needs database credentials and API keys."
      },
      {
        "id": "aws-003",
        "topic": "aws",
        "difficulty": "hard",
        "question": "Your application running on EC2 is experiencing high latency when accessing S3. What should you investigate first?",
        "options": [
          "EC2 instance type",
          "S3 bucket region vs EC2 region",
          "Application code efficiency",
          "Network ACLs"
        ],
        "correct_answer": 2,
        "explanation": "Cross-region data transfer introduces significant latency. S3 buckets and EC2 instances should be in the same region for optimal performance.",
        "scenario": "Your web application loads images from S3, but users are reporting slow page load times."
      },
      {
        "id": "k8s-001",
        "topic": "kubernetes",
        "difficulty": "easy",
        "question": "What is a Pod in Kubernetes?",
        "options": [
          "A collection of nodes",
          "The smallest deployable unit that contains one or more containers",
          "A type of service",
          "A configuration file"
        ],
        "correct_answer": 2,
        "explanation": "A Pod is the smallest and simplest unit in the Kubernetes object model that represents a set of running containers on your cluster."
      },
      {
        "id": "k8s-002",
        "topic": "kubernetes",
        "difficulty": "medium",
        "question": "How do you expose a deployment to external traffic?",
        "options": [
          "Create a Pod",
          "Create a Service with type LoadBalancer or NodePort",
          "Create a ConfigMap",
          "Create a Secret"
        ],
        "correct_answer": 2,
        "explanation": "Services with type LoadBalancer or NodePort expose pods to external traffic. Ingress controllers are another option for HTTP/HTTPS traffic.",
        "scenario": "You have a web application deployment running in your cluster that needs to be accessible from the internet."
      },
      {
        "id": "k8s-003",
        "topic": "kubernetes",
        "difficulty": "hard",
        "question": "Your pods are being evicted with 'disk pressure' errors. What's the most likely cause?",
        "options": [
          "Insufficient CPU resources",
          "Insufficient memory resources",
          "Node running out of disk space",
          "Network connectivity issues"
        ],
        "correct_answer": 3,
        "explanation": "Disk pressure occurs when a node's available disk space falls below a threshold. This triggers pod eviction to prevent node failure.",
        "scenario": "You notice several pods in your application are being terminated unexpectedly with eviction events."
      },
      {
        "id": "docker-001",
        "topic": "docker",
        "difficulty": "easy",
        "question": "What command creates and starts a new container?",
        "options": [
          "docker create",
          "docker start",
          "docker run",
          "docker exec"
        ],
        "correct_answer": 3,
        "explanation": "The 'docker run' command creates a new container from an image and starts it. It combines 'docker create' and 'docker start'."
      },
      {
        "id": "docker-002",
        "topic": "docker",
        "difficulty": "medium",
        "question": "How do you persist data beyond a container's lifecycle?",
        "options": [
          "Store data in the container filesystem",
          "Use volumes or bind mounts",
          "Copy files after container stops",
          "Use environment variables"
        ],
        "correct_answer": 2,
        "explanation": "Volumes and bind mounts allow data to persist beyond the container's lifecycle. Volumes are managed by Docker, while bind mounts reference host filesystem paths.",
        "scenario": "You need to ensure database data survives container restarts and updates."
      },
      {
        "id": "docker-003",
        "topic": "docker",
        "difficulty": "hard",
        "question": "Your Docker build is slow and creates large images. What optimization should you prioritize?",
        "options": [
          "Use smaller base images and multi-stage builds",
          "Increase build memory",
          "Use faster storage",
          "Build on more powerful hardware"
        ],
        "correct_answer": 1,
        "explanation": "Using smaller base images (like Alpine) and multi-stage builds dramatically reduces image size and build time by eliminating unnecessary dependencies and intermediate layers.",
        "scenario": "Your Node.js application Docker image is 2GB and takes 15 minutes to build."
      },
      {
        "id": "linux-001",
        "topic": "linux",
        "difficulty": "easy",
        "question": "Which command shows running processes?",
        "options": [
          "ls",
          "ps",
          "cd",
          "cp"
        ],
        "correct_answer": 2,
        "explanation": "The 'ps' command displays information about running processes. Common options include 'ps aux' for detailed process information."
      },
      {
        "id": "linux-002",
        "topic": "linux",
        "difficulty": "medium",
        "question": "How do you find files modified in the last 7 days?",
        "options": [
          "find . -mtime -7",
          "find . -mtime +7",
          "ls -la",
          "grep -r"
        ],
        "correct_answer": 1,
        "explanation": "The 'find' command with '-mtime -7' finds files modified less than 7 days ago. The minus sign means 'less than'.",
        "scenario": "You need to identify recently changed configuration files to troubleshoot a service issue."
      },
      {
        "id": "git-001",
        "topic": "git",
        "difficulty": "easy",
        "question": "Which command stages all changes for commit?",
        "options": [
          "git commit -a",
          "git add .",
          "git push",
          "git pull"
        ],
        "correct_answer": 2,
        "explanation": "The 'git add .' command stages all changes in the current directory and subdirectories for the next commit."
      },
      {
        "id": "git-002",
        "topic": "git",
        "difficulty": "medium",
        "question": "How do you undo the last commit but keep the changes?",
        "options": [
          "git reset --hard HEAD~1",
          "git reset --soft HEAD~1",
          "git revert HEAD",
          "git checkout HEAD~1"
        ],
        "correct_answer": 2,
        "explanation": "The 'git reset --soft HEAD~1' moves the HEAD pointer back one commit but keeps all changes staged. '--hard' would discard changes.",
        "scenario": "You committed changes but realized you forgot to include an important file."
      },
      {
        "id": "networking-001",
        "topic": "networking",
        "difficulty": "easy",
        "question": "What is the default port for HTTP?",
        "options": [
          "21",
          "22",
          "80",
          "443"
        ],
        "correct_answer": 3,
        "explanation": "HTTP uses port 80 by default. HTTPS uses port 443, SSH uses port 22, and FTP uses port 21."
      },
      {
        "id": "networking-002",
        "topic": "networking",
        "difficulty": "medium",
        "question": "What tool helps diagnose network connectivity issues by showing the path packets take?",
        "options": [
          "ping",
          "traceroute",
          "netstat",
          "ss"
        ],
        "correct_answer": 2,
        "explanation": "Traceroute shows the path packets take to reach a destination, displaying each hop and latency. This helps identify where network issues occur.",
        "scenario": "Users report intermittent connectivity issues to your web application."
      },
      {
        "id": "terraform-001",
        "topic": "terraform",
        "difficulty": "easy",
        "question": "What command applies Terraform configuration changes?",
        "options": [
          "terraform init",
          "terraform plan",
          "terraform apply",
          "terraform destroy"
        ],
        "correct_answer": 3,
        "explanation": "The 'terraform apply' command executes the planned changes to reach the desired state defined in your configuration files."
      },
      {
        "id": "terraform-002",
        "topic": "terraform",
        "difficulty": "medium",
        "question": "How do you prevent accidental deletion of critical resources?",
        "options": [
          "Use terraform destroy --force",
          "Add lifecycle { prevent_destroy = true }",
          "Use terraform import",
          "Create multiple workspaces"
        ],
        "correct_answer": 2,
        "explanation": "The lifecycle block with prevent_destroy = true prevents Terraform from destroying the resource, even if it's removed from configuration.",
        "scenario": "You have a production database that must never be accidentally deleted during infrastructure updates."
      },
      {
        "id": "cicd-001",
        "topic": "cicd",
        "difficulty": "easy",
        "question": "What triggers a GitHub Actions workflow?",
        "options": [
          "Manual execution only",
          "Events like push, pull_request, or schedule",
          "Email notifications",
          "SSH connections"
        ],
        "correct_answer": 2,
        "explanation": "GitHub Actions workflows are triggered by various events such as push, pull_request, schedule, or manual workflow_dispatch."
      },
      {
        "id": "cicd-002",
        "topic": "cicd",
        "difficulty": "medium",
        "question": "What's the best practice for handling secrets in GitHub Actions?",
        "options": [
          "Store in workflow files",
          "Use GitHub Secrets and reference with ${{ secrets.SECRET_NAME }}",
          "Store in repository README",
          "Hardcode in application"
        ],
        "correct_answer": 2,
        "explanation": "GitHub Secrets provide secure storage for sensitive information and can be referenced in workflows using the secrets context.",
        "scenario": "Your CI/CD pipeline needs to deploy to AWS using access keys."
      },
      {
        "id": "cicd-003",
        "topic": "cicd",
        "difficulty": "hard",
        "question": "Your GitHub Actions workflow fails intermittently on the same step. What's the best debugging approach?",
        "options": [
          "Ignore the failures",
          "Add debug logging and use actions/upload-artifact for troubleshooting",
          "Disable the failing step",
          "Run the workflow more frequently"
        ],
        "correct_answer": 2,
        "explanation": "Adding debug logging (using echo or actions/debug) and uploading artifacts helps identify the root cause of intermittent failures.",
        "scenario": "Your test step passes locally but fails randomly in CI, causing deployment delays."
      },
      {
        "id": "aws-004",
        "topic": "aws",
        "difficulty": "easy",
        "question": "Which AWS service is used for DNS management?",
        "options": [
          "CloudFront",
          "Route 53",
          "ELB",
          "API Gateway"
        ],
        "correct_answer": 2,
        "explanation": "Amazon Route 53 is AWS's scalable DNS web service that translates domain names into IP addresses."
      },
      {
        "id": "aws-005",
        "topic": "aws",
        "difficulty": "medium",
        "question": "You need to automatically scale EC2 instances based on CPU utilization. Which service combination should you use?",
        "options": [
          "CloudWatch + Auto Scaling Groups",
          "Lambda + CloudFormation",
          "ECS + Fargate",
          "S3 + CloudFront"
        ],
        "correct_answer": 1,
        "explanation": "CloudWatch monitors metrics like CPU utilization, and Auto Scaling Groups automatically add or remove EC2 instances based on those metrics.",
        "scenario": "Your e-commerce website experiences traffic spikes during sales events and you want to handle load automatically."
      },
      {
        "id": "aws-006",
        "topic": "aws",
        "difficulty": "hard",
        "question": "Your Lambda function is timing out when processing large files from S3. What's the most cost-effective solution?",
        "options": [
          "Increase Lambda memory and timeout",
          "Use Step Functions to break processing into chunks",
          "Switch to EC2 instances",
          "Use SQS to queue the processing"
        ],
        "correct_answer": 2,
        "explanation": "Step Functions allow you to break large processing tasks into smaller, manageable chunks, avoiding Lambda timeout limits while maintaining serverless architecture.",
        "scenario": "You're processing 1GB video files for transcoding, but Lambda times out after 15 minutes."
      },
      {
        "id": "aws-007",
        "topic": "aws",
        "difficulty": "hard",
        "question": "Interview Question: How would you design a disaster recovery strategy for a critical application with RTO of 1 hour and RPO of 15 minutes?",
        "options": [
          "Single AZ deployment with daily backups",
          "Multi-AZ RDS with cross-region read replicas and automated failover",
          "S3 versioning with lifecycle policies",
          "EBS snapshots every hour"
        ],
        "correct_answer": 2,
        "explanation": "Multi-AZ RDS provides automated failover within minutes, and cross-region read replicas can be promoted to master for disaster recovery, meeting both RTO and RPO requirements.",
        "scenario": "Your banking application cannot afford more than 1 hour downtime and can only lose maximum 15 minutes of data."
      },
      {
        "id": "k8s-004",
        "topic": "kubernetes",
        "difficulty": "easy",
        "question": "What is the default restart policy for pods in Kubernetes?",
        "options": [
          "Never",
          "OnFailure",
          "Always",
          "Manual"
        ],
        "correct_answer": 3,
        "explanation": "The default restart policy for pods is 'Always', meaning containers will be restarted regardless of exit status."
      },
      {
        "id": "k8s-005",
        "topic": "kubernetes",
        "difficulty": "medium",
        "question": "Your application pods need to communicate with each other by name. What Kubernetes resource should you create?",
        "options": [
          "Ingress",
          "Service",
          "ConfigMap",
          "PersistentVolume"
        ],
        "correct_answer": 2,
        "explanation": "Services provide stable DNS names and load balancing for pods, allowing them to communicate reliably even as pods are created and destroyed.",
        "scenario": "You have a microservices architecture where the frontend needs to call the backend API by a consistent hostname."
      },
      {
        "id": "k8s-006",
        "topic": "kubernetes",
        "difficulty": "hard",
        "question": "Interview Question: Your cluster has nodes with different resource capacities. How do you ensure critical pods are scheduled on nodes with sufficient resources?",
        "options": [
          "Use resource requests and limits with node selectors",
          "Manually assign pods to nodes",
          "Use only horizontal pod autoscaling",
          "Increase cluster size"
        ],
        "correct_answer": 1,
        "explanation": "Resource requests ensure the scheduler only places pods on nodes with sufficient available resources, while limits prevent resource overconsumption. Node selectors can target specific node types.",
        "scenario": "You have a mix of CPU-intensive and memory-intensive applications that need to be scheduled appropriately across heterogeneous nodes."
      },
      {
        "id": "k8s-007",
        "topic": "kubernetes",
        "difficulty": "hard",
        "question": "Your cluster is experiencing intermittent DNS resolution failures. What should you investigate first?",
        "options": [
          "Node network configuration",
          "CoreDNS pod status and logs",
          "Ingress controller configuration",
          "Pod security policies"
        ],
        "correct_answer": 2,
        "explanation": "CoreDNS handles DNS resolution in Kubernetes clusters. DNS failures are often caused by CoreDNS pods being unhealthy, resource-constrained, or misconfigured.",
        "scenario": "Applications randomly fail to resolve service names, causing intermittent connection failures."
      },
      {
        "id": "docker-004",
        "topic": "docker",
        "difficulty": "easy",
        "question": "Which Docker command shows all running containers?",
        "options": [
          "docker ls",
          "docker ps",
          "docker list",
          "docker show"
        ],
        "correct_answer": 2,
        "explanation": "The 'docker ps' command lists all currently running containers. Use 'docker ps -a' to see all containers including stopped ones."
      },
      {
        "id": "docker-005",
        "topic": "docker",
        "difficulty": "medium",
        "question": "What's the difference between COPY and ADD in a Dockerfile?",
        "options": [
          "No difference, they're aliases",
          "COPY is for files, ADD is for directories",
          "ADD has additional features like extracting archives and downloading URLs",
          "COPY is faster than ADD"
        ],
        "correct_answer": 3,
        "explanation": "ADD has additional features like automatically extracting compressed archives and downloading files from URLs, while COPY is simpler and more predictable for basic file copying.",
        "scenario": "You're deciding between COPY and ADD for your Dockerfile and want to follow best practices."
      },
      {
        "id": "docker-006",
        "topic": "docker",
        "difficulty": "hard",
        "question": "Interview Question: Your containerized application works locally but fails in production with permission errors. What's the most likely cause?",
        "options": [
          "Different Docker versions",
          "Container running as root locally but non-root in production",
          "Missing environment variables",
          "Network configuration differences"
        ],
        "correct_answer": 2,
        "explanation": "Permission errors often occur when containers run as different users between environments. Production environments typically run containers as non-root for security, while local development might default to root.",
        "scenario": "Your application can write to files locally but gets 'Permission denied' errors when deployed to production Kubernetes cluster."
      },
      {
        "id": "linux-003",
        "topic": "linux",
        "difficulty": "easy",
        "question": "Which command displays the current working directory?",
        "options": [
          "pwd",
          "cwd",
          "dir",
          "where"
        ],
        "correct_answer": 1,
        "explanation": "The 'pwd' (print working directory) command displays the full pathname of the current directory."
      },
      {
        "id": "linux-004",
        "topic": "linux",
        "difficulty": "medium",
        "question": "How do you run a command in the background and keep it running after logout?",
        "options": [
          "command &",
          "nohup command &",
          "bg command",
          "screen command"
        ],
        "correct_answer": 2,
        "explanation": "The 'nohup' command runs a process immune to hangups (continues after logout), and '&' runs it in the background.",
        "scenario": "You need to run a long data processing script on a remote server and disconnect your SSH session."
      },
      {
        "id": "linux-005",
        "topic": "linux",
        "difficulty": "hard",
        "question": "Interview Question: A process is consuming 100% CPU and you can't kill it with 'kill -9'. What could be the cause?",
        "options": [
          "Process has too many child processes",
          "Process is in uninterruptible sleep (D state)",
          "Insufficient permissions",
          "Process is already dead"
        ],
        "correct_answer": 2,
        "explanation": "Processes in uninterruptible sleep (D state) are usually waiting for I/O operations and cannot be killed until the I/O completes. This often indicates hardware or kernel issues.",
        "scenario": "A process shows as consuming resources but won't respond to any kill signals, even SIGKILL."
      },
      {
        "id": "git-003",
        "topic": "git",
        "difficulty": "easy",
        "question": "Which command shows the commit history?",
        "options": [
          "git history",
          "git log",
          "git show",
          "git commits"
        ],
        "correct_answer": 2,
        "explanation": "The 'git log' command displays the commit history showing commit hashes, authors, dates, and commit messages."
      },
      {
        "id": "git-004",
        "topic": "git",
        "difficulty": "medium",
        "question": "You accidentally committed sensitive data. How do you remove it from Git history?",
        "options": [
          "git rm filename && git commit",
          "git revert HEAD",
          "git filter-branch or git filter-repo",
          "Delete the repository and start over"
        ],
        "correct_answer": 3,
        "explanation": "git filter-branch (or the newer git filter-repo) can rewrite Git history to completely remove sensitive data from all commits, not just the latest one.",
        "scenario": "You accidentally committed API keys to your repository and need to remove them from all historical commits."
      },
      {
        "id": "git-005",
        "topic": "git",
        "difficulty": "hard",
        "question": "Interview Question: Your team has a merge conflict in a critical production branch. Walk through your resolution strategy.",
        "options": [
          "Force push to overwrite conflicts",
          "Identify conflicting files, manually resolve conflicts, test thoroughly, then commit",
          "Delete the branch and recreate it",
          "Use git reset --hard to discard changes"
        ],
        "correct_answer": 2,
        "explanation": "Proper merge conflict resolution involves: 1) Identify conflicts with git status, 2) Manually edit files to resolve conflicts, 3) Test the resolution thoroughly, 4) Stage resolved files, 5) Commit the merge resolution.",
        "scenario": "Two developers modified the same critical configuration file and you need to safely merge their changes for a production deployment."
      },
      {
        "id": "networking-003",
        "topic": "networking",
        "difficulty": "easy",
        "question": "What does HTTPS stand for?",
        "options": [
          "HyperText Transfer Protocol Secure",
          "HyperText Transport Protocol Secure",
          "HyperText Transmission Protocol Secure",
          "HyperText Transfer Protocol System"
        ],
        "correct_answer": 1,
        "explanation": "HTTPS stands for HyperText Transfer Protocol Secure, which is HTTP over TLS/SSL encryption."
      },
      {
        "id": "networking-004",
        "topic": "networking",
        "difficulty": "medium",
        "question": "Your web application is slow. How do you determine if it's a network or application issue?",
        "options": [
          "Restart the application",
          "Use tools like ping, curl -w, and application monitoring",
          "Increase server resources",
          "Check only application logs"
        ],
        "correct_answer": 2,
        "explanation": "Use ping for basic connectivity, curl with timing (-w) to measure HTTP response times, and application monitoring to distinguish between network latency and application processing time.",
        "scenario": "Users complain about slow page load times and you need to identify whether the bottleneck is network or application performance."
      },
      {
        "id": "networking-005",
        "topic": "networking",
        "difficulty": "hard",
        "question": "Interview Question: Design a network architecture for a web application that needs to handle 1M requests per day with high availability.",
        "options": [
          "Single server with powerful hardware",
          "Load balancer + multiple app servers + database clustering + CDN",
          "Serverless functions only",
          "Container orchestration without load balancing"
        ],
        "correct_answer": 2,
        "explanation": "High availability requires redundancy at every layer: load balancers for traffic distribution, multiple application servers, database clustering for data redundancy, and CDN for global performance.",
        "scenario": "You're architecting a new e-commerce platform that must handle Black Friday traffic spikes without downtime."
      },
      {
        "id": "terraform-003",
        "topic": "terraform",
        "difficulty": "easy",
        "question": "What file extension do Terraform configuration files use?",
        "options": [
          ".tf",
          ".terraform",
          ".hcl",
          ".tfconfig"
        ],
        "correct_answer": 1,
        "explanation": "Terraform configuration files use the .tf extension and are written in HashiCorp Configuration Language (HCL)."
      },
      {
        "id": "terraform-004",
        "topic": "terraform",
        "difficulty": "medium",
        "question": "How do you manage different environments (dev, staging, prod) in Terraform?",
        "options": [
          "Separate .tf files for each environment",
          "Use workspaces or separate state files with variables",
          "Hard-code environment-specific values",
          "Use different Terraform versions"
        ],
        "correct_answer": 2,
        "explanation": "Terraform workspaces or separate state files combined with environment-specific variable files allow you to manage multiple environments safely without code duplication.",
        "scenario": "You need to deploy the same infrastructure to development, staging, and production with different configurations."
      },
      {
        "id": "terraform-005",
        "topic": "terraform",
        "difficulty": "hard",
        "question": "Interview Question: Your Terraform state file is corrupted and out of sync with actual infrastructure. How do you recover?",
        "options": [
          "Delete all infrastructure and recreate",
          "Use terraform import to reconcile existing resources with state",
          "Manually edit the state file",
          "Ignore the state file and create new resources"
        ],
        "correct_answer": 2,
        "explanation": "terraform import allows you to bring existing infrastructure under Terraform management by importing resources into the state file, reconciling the state with reality.",
        "scenario": "After a failed deployment, your state file shows resources that don't exist, and real resources that aren't tracked in state."
      },
      {
        "id": "cicd-004",
        "topic": "cicd",
        "difficulty": "easy",
        "question": "What does CI stand for in CI/CD?",
        "options": [
          "Code Integration",
          "Continuous Integration",
          "Computer Integration",
          "Container Integration"
        ],
        "correct_answer": 2,
        "explanation": "CI stands for Continuous Integration, the practice of frequently integrating code changes into a shared repository with automated testing."
      },
      {
        "id": "cicd-005",
        "topic": "cicd",
        "difficulty": "medium",
        "question": "Your GitHub Actions workflow needs to deploy to different environments based on the branch. How do you implement this?",
        "options": [
          "Create separate workflows for each branch",
          "Use conditional steps with if: statements and environment variables",
          "Manually trigger deployments",
          "Use the same deployment for all branches"
        ],
        "correct_answer": 2,
        "explanation": "Conditional steps using 'if' statements can check the branch name and set appropriate environment variables to control deployment targets.",
        "scenario": "You want main branch to deploy to production, develop branch to staging, and feature branches to development environment."
      },
      {
        "id": "cicd-006",
        "topic": "cicd",
        "difficulty": "hard",
        "question": "Interview Question: Your deployment pipeline needs zero-downtime deployments. What strategy would you implement?",
        "options": [
          "Stop all services, deploy, restart services",
          "Blue-green deployment or rolling updates with health checks",
          "Deploy during maintenance windows only",
          "Use feature flags to hide new features"
        ],
        "correct_answer": 2,
        "explanation": "Blue-green deployments maintain two identical environments, switching traffic after deployment validation. Rolling updates gradually replace instances while maintaining service availability.",
        "scenario": "Your e-commerce platform cannot afford any downtime during deployments, especially during peak shopping hours."
      },
      {
        "id": "security-001",
        "topic": "security",
        "difficulty": "easy",
        "question": "What is the principle of least privilege?",
        "options": [
          "Give users maximum permissions for convenience",
          "Grant minimum permissions necessary to perform job functions",
          "Use the same password for all accounts",
          "Allow anonymous access to reduce complexity"
        ],
        "correct_answer": 2,
        "explanation": "The principle of least privilege means granting users, processes, and systems only the minimum permissions necessary to perform their required functions."
      },
      {
        "id": "security-002",
        "topic": "security",
        "difficulty": "medium",
        "question": "How should you handle API keys in a containerized application?",
        "options": [
          "Hard-code them in the Docker image",
          "Pass them as environment variables or use secret management systems",
          "Store them in the application logs",
          "Include them in the source code"
        ],
        "correct_answer": 2,
        "explanation": "API keys should be passed as environment variables at runtime or retrieved from dedicated secret management systems like Kubernetes Secrets, AWS Secrets Manager, or HashiCorp Vault.",
        "scenario": "Your microservice needs to authenticate with external APIs while maintaining security best practices."
      },
      {
        "id": "security-003",
        "topic": "security",
        "difficulty": "hard",
        "question": "Interview Question: You discover a security vulnerability in your production system. Walk through your incident response process.",
        "options": [
          "Fix it immediately without documentation",
          "Assess impact, contain threat, notify stakeholders, fix vulnerability, document and review",
          "Wait for the next scheduled maintenance window",
          "Only fix if actively being exploited"
        ],
        "correct_answer": 2,
        "explanation": "Proper incident response: 1) Assess impact and severity, 2) Contain the threat, 3) Notify relevant stakeholders, 4) Implement fix, 5) Document incident and lessons learned, 6) Review and improve processes.",
        "scenario": "Your security team reports a critical vulnerability in a library used by your production application that processes customer data."
      },
      {
        "id": "monitoring-001",
        "topic": "monitoring",
        "difficulty": "easy",
        "question": "What are the three pillars of observability?",
        "options": [
          "CPU, Memory, Disk",
          "Logs, Metrics, Traces",
          "Alerts, Dashboards, Reports",
          "Monitoring, Alerting, Reporting"
        ],
        "correct_answer": 2,
        "explanation": "The three pillars of observability are Logs (detailed records), Metrics (numerical measurements), and Traces (request flows through systems)."
      },
      {
        "id": "monitoring-002",
        "topic": "monitoring",
        "difficulty": "medium",
        "question": "Your application response time suddenly increased. What metrics should you check first?",
        "options": [
          "Only application logs",
          "CPU usage, memory usage, disk I/O, network latency, and database performance",
          "Just restart the application",
          "Check only user complaints"
        ],
        "correct_answer": 2,
        "explanation": "Performance issues can stem from resource constraints (CPU, memory), I/O bottlenecks (disk, network), or database problems. Check system metrics before diving into application-specific issues.",
        "scenario": "Your web application's average response time jumped from 200ms to 2 seconds and users are complaining about slowness."
      },
      {
        "id": "monitoring-003",
        "topic": "monitoring",
        "difficulty": "hard",
        "question": "Interview Question: Design a monitoring strategy for a microservices architecture with 20+ services.",
        "options": [
          "Monitor each service independently",
          "Centralized logging, distributed tracing, service mesh metrics, and SLA-based alerting",
          "Only monitor the main application",
          "Use basic uptime checks only"
        ],
        "correct_answer": 2,
        "explanation": "Microservices require: centralized logging for correlation, distributed tracing to follow requests across services, service mesh metrics for inter-service communication, and SLA-based alerting focused on user experience.",
        "scenario": "You need to ensure visibility into a complex microservices system where a single user request might touch 10+ different services."
      },
      {
        "id": "linux-006",
        "topic": "linux",
        "difficulty": "easy",
        "question": "What is the Linux kernel?",
        "options": [
          "A text editor for Linux",
          "The core component that manages system resources and hardware",
          "A graphical user interface",
          "A package manager"
        ],
        "correct_answer": 2,
        "explanation": "The Linux kernel is the core component of the Linux operating system that manages system resources, hardware, and provides an interface between hardware and software."
      },
      {
        "id": "linux-007",
        "topic": "linux",
        "difficulty": "medium",
        "question": "What are inodes in Linux?",
        "options": [
          "Network interfaces",
          "Data structures that store metadata about files and directories",
          "Process identifiers",
          "Memory addresses"
        ],
        "correct_answer": 2,
        "explanation": "Inodes are data structures that store metadata about files and directories, including permissions, ownership, timestamps, and pointers to data blocks.",
        "scenario": "You're investigating filesystem issues and need to understand how Linux tracks file information."
      },
      {
        "id": "linux-008",
        "topic": "linux",
        "difficulty": "hard",
        "question": "What is a zombie process and how would you handle it?",
        "options": [
          "A process that consumes too much memory",
          "A child process that has completed but parent hasn't read its exit status",
          "A process running in the background",
          "A process with root privileges"
        ],
        "correct_answer": 2,
        "explanation": "A zombie process is a child process that has completed execution but its parent hasn't read its exit status yet. The process entry remains in the process table until the parent reads it.",
        "scenario": "You notice zombie processes accumulating on your system and need to understand why they're not being cleaned up."
      },
      {
        "id": "linux-009",
        "topic": "linux",
        "difficulty": "medium",
        "question": "What is the difference between soft links and hard links?",
        "options": [
          "No difference, they're the same",
          "Soft links point to filenames, hard links point to inodes directly",
          "Hard links are slower than soft links",
          "Soft links only work with directories"
        ],
        "correct_answer": 2,
        "explanation": "Soft links (symbolic links) point to filenames and can break if the target is moved/deleted. Hard links point directly to inodes and remain valid as long as the file exists.",
        "scenario": "You need to create links to important files and want to understand which type is more appropriate for your use case."
      },
      {
        "id": "linux-010",
        "topic": "linux",
        "difficulty": "easy",
        "question": "What does chmod +x FILENAME do?",
        "options": [
          "Makes the file hidden",
          "Adds execute permission to the file",
          "Changes file ownership",
          "Compresses the file"
        ],
        "correct_answer": 2,
        "explanation": "chmod +x adds execute permission to a file, allowing it to be run as a program or script."
      },
      {
        "id": "linux-011",
        "topic": "linux",
        "difficulty": "easy",
        "question": "Which command shows free/used memory?",
        "options": [
          "df -h",
          "free -h",
          "ps aux",
          "top"
        ],
        "correct_answer": 2,
        "explanation": "The 'free -h' command displays memory usage in human-readable format, showing total, used, free, and available memory."
      },
      {
        "id": "linux-012",
        "topic": "linux",
        "difficulty": "medium",
        "question": "What is swap space in Linux?",
        "options": [
          "Temporary storage for downloads",
          "Virtual memory that extends RAM using disk space",
          "Cache for frequently accessed files",
          "Space for log files"
        ],
        "correct_answer": 2,
        "explanation": "Swap space is virtual memory that uses disk space to extend RAM when physical memory is full, allowing the system to handle more processes.",
        "scenario": "Your system is running out of memory and you need to understand how swap can help manage memory pressure."
      },
      {
        "id": "linux-013",
        "topic": "linux",
        "difficulty": "hard",
        "question": "What is inside /proc in Linux?",
        "options": [
          "User programs and applications",
          "Virtual filesystem containing kernel and process information",
          "System configuration files",
          "Log files and temporary data"
        ],
        "correct_answer": 2,
        "explanation": "/proc is a virtual filesystem that provides an interface to kernel data structures and runtime system information, including process details, system statistics, and kernel parameters.",
        "scenario": "You need to gather detailed system information for debugging and want to understand what /proc contains."
      },
      {
        "id": "networking-006",
        "topic": "networking",
        "difficulty": "medium",
        "question": "How does a 3-way handshake work in TCP?",
        "options": [
          "Client sends data, server responds, client acknowledges",
          "SYN, SYN-ACK, ACK sequence to establish connection",
          "Server sends request, client responds, server confirms",
          "Three separate connections are established"
        ],
        "correct_answer": 2,
        "explanation": "TCP 3-way handshake: 1) Client sends SYN packet, 2) Server responds with SYN-ACK, 3) Client sends ACK to establish the connection.",
        "scenario": "You're debugging connection issues and need to understand how TCP connections are established."
      },
      {
        "id": "networking-007",
        "topic": "networking",
        "difficulty": "hard",
        "question": "When I type google.com into the browser, what actually happens?",
        "options": [
          "Browser directly connects to Google's servers",
          "DNS resolution, TCP handshake, HTTP request, response rendering",
          "Only HTTP request is sent",
          "Browser cache lookup only"
        ],
        "correct_answer": 2,
        "explanation": "Complete flow: DNS resolution to get IP, TCP 3-way handshake, TLS handshake if HTTPS, HTTP request sent, server processes and responds, browser renders the page.",
        "scenario": "Classic interview question testing understanding of the complete web request flow from browser to server."
      },
      {
        "id": "networking-008",
        "topic": "networking",
        "difficulty": "medium",
        "question": "What is the difference between TCP and UDP?",
        "options": [
          "TCP is faster than UDP",
          "TCP is connection-oriented and reliable, UDP is connectionless and faster",
          "UDP is more secure than TCP",
          "TCP only works with HTTP"
        ],
        "correct_answer": 2,
        "explanation": "TCP provides reliable, ordered delivery with connection management and error correction. UDP is connectionless, faster, but doesn't guarantee delivery or order.",
        "scenario": "You're designing a system and need to choose between TCP and UDP based on your requirements."
      },
      {
        "id": "aws-008",
        "topic": "aws",
        "difficulty": "medium",
        "question": "What is an AMI in AWS?",
        "options": [
          "Amazon Machine Image - a template for EC2 instances",
          "A monitoring service",
          "A database backup",
          "A load balancer configuration"
        ],
        "correct_answer": 1,
        "explanation": "An AMI (Amazon Machine Image) is a template that contains the software configuration (OS, application server, applications) required to launch an EC2 instance.",
        "scenario": "You need to create standardized server configurations for your development team."
      },
      {
        "id": "aws-009",
        "topic": "aws",
        "difficulty": "hard",
        "question": "I want to create a 3-tier architecture. What would be the key components?",
        "options": [
          "Three EC2 instances in different regions",
          "Presentation tier (ALB/CloudFront), Logic tier (EC2/ECS), Data tier (RDS/DynamoDB)",
          "Three separate VPCs",
          "Three different AWS accounts"
        ],
        "correct_answer": 2,
        "explanation": "3-tier architecture: Presentation tier (load balancers, CDN), Application/Logic tier (compute services), Data tier (databases). Each tier can scale independently.",
        "scenario": "You're designing a scalable web application architecture that separates concerns and allows independent scaling."
      },
      {
        "id": "aws-010",
        "topic": "aws",
        "difficulty": "easy",
        "question": "What is auto-scaling in AWS?",
        "options": [
          "Automatic cost optimization",
          "Automatically adjusting the number of EC2 instances based on demand",
          "Automatic backup scheduling",
          "Automatic security updates"
        ],
        "correct_answer": 2,
        "explanation": "Auto-scaling automatically adjusts the number of EC2 instances in response to changing demand, helping maintain performance while optimizing costs."
      },
      {
        "id": "docker-007",
        "topic": "docker",
        "difficulty": "medium",
        "question": "What is the difference between COPY and ADD in a Dockerfile?",
        "options": [
          "COPY is newer than ADD",
          "ADD has additional features like URL downloads and archive extraction",
          "COPY is faster than ADD",
          "No difference, they do the same thing"
        ],
        "correct_answer": 2,
        "explanation": "ADD can download files from URLs and automatically extract archives, while COPY only copies files/folders. COPY is preferred for simple file copying due to its predictable behavior."
      },
      {
        "id": "docker-008",
        "topic": "docker",
        "difficulty": "medium",
        "question": "What is the difference between CMD and RUN in a Dockerfile?",
        "options": [
          "No difference, they're aliases",
          "RUN executes during build time, CMD specifies default command at runtime",
          "CMD is only for Linux containers",
          "RUN is deprecated, use CMD instead"
        ],
        "correct_answer": 2,
        "explanation": "RUN executes commands during the image build process and creates new layers. CMD specifies the default command to run when a container starts.",
        "scenario": "You're writing a Dockerfile and need to understand when to use RUN vs CMD for different operations."
      },
      {
        "id": "docker-009",
        "topic": "docker",
        "difficulty": "easy",
        "question": "What is a dangling image in Docker?",
        "options": [
          "An image that's currently running",
          "An untagged image that's no longer referenced by any tagged image",
          "A corrupted image file",
          "An image stored remotely"
        ],
        "correct_answer": 2,
        "explanation": "A dangling image is an untagged image that's no longer referenced by any tagged image, often created during the build process. They can be cleaned up with 'docker image prune'."
      },
      {
        "id": "k8s-008",
        "topic": "kubernetes",
        "difficulty": "easy",
        "question": "What problems does Kubernetes solve?",
        "options": [
          "Only container security",
          "Container orchestration, scaling, service discovery, and management",
          "Only load balancing",
          "Database management only"
        ],
        "correct_answer": 2,
        "explanation": "Kubernetes solves container orchestration challenges including automated deployment, scaling, load balancing, service discovery, health monitoring, and rolling updates."
      },
      {
        "id": "k8s-009",
        "topic": "kubernetes",
        "difficulty": "medium",
        "question": "What are deployments in Kubernetes?",
        "options": [
          "Single pod configurations",
          "Objects that manage ReplicaSets and provide declarative updates to pods",
          "Network policies",
          "Storage configurations"
        ],
        "correct_answer": 2,
        "explanation": "Deployments manage ReplicaSets and provide declarative updates to pods, enabling rolling updates, rollbacks, and scaling of applications.",
        "scenario": "You need to deploy an application with multiple replicas and want automatic rolling updates."
      },
      {
        "id": "k8s-010",
        "topic": "kubernetes",
        "difficulty": "hard",
        "question": "What happens when a master node fails in Kubernetes?",
        "options": [
          "All pods stop immediately",
          "Existing pods continue running but no new scheduling or API operations",
          "The cluster shuts down completely",
          "Worker nodes take over master functions"
        ],
        "correct_answer": 2,
        "explanation": "When master node fails, existing pods continue running on worker nodes, but no new scheduling, scaling, or API operations can occur until the master is restored or fails over to backup masters.",
        "scenario": "You're designing a highly available Kubernetes cluster and need to understand failure scenarios."
      },
      {
        "id": "terraform-006",
        "topic": "terraform",
        "difficulty": "easy",
        "question": "What is Infrastructure as Code (IaC)?",
        "options": [
          "Writing code inside infrastructure",
          "Managing infrastructure through machine-readable definition files",
          "Coding applications for cloud platforms",
          "Manual infrastructure configuration"
        ],
        "correct_answer": 2,
        "explanation": "Infrastructure as Code (IaC) is the practice of managing and provisioning infrastructure through machine-readable definition files, rather than manual processes."
      },
      {
        "id": "terraform-007",
        "topic": "terraform",
        "difficulty": "medium",
        "question": "What are modules in Terraform?",
        "options": [
          "Individual resource definitions",
          "Reusable packages of Terraform configuration",
          "State file backups",
          "Provider plugins"
        ],
        "correct_answer": 2,
        "explanation": "Modules are reusable packages of Terraform configuration that encapsulate resources and can be shared across different projects or environments.",
        "scenario": "You want to create reusable infrastructure components that can be shared across multiple projects."
      },
      {
        "id": "terraform-008",
        "topic": "terraform",
        "difficulty": "hard",
        "question": "How can you import existing resources under Terraform management?",
        "options": [
          "Delete and recreate the resources",
          "Use 'terraform import' command to bring existing resources into state",
          "Manually edit the state file",
          "Use 'terraform apply' with existing resources"
        ],
        "correct_answer": 2,
        "explanation": "The 'terraform import' command allows you to bring existing infrastructure resources under Terraform management by importing them into the state file.",
        "scenario": "You have existing infrastructure that wasn't created with Terraform and want to manage it with Terraform going forward."
      },
      {
        "id": "git-006",
        "topic": "git",
        "difficulty": "easy",
        "question": "What is the basic Git workflow?",
        "options": [
          "Edit, commit, push",
          "Add, commit, push",
          "Clone, edit, push",
          "Pull, edit, commit, push"
        ],
        "correct_answer": 4,
        "explanation": "Basic Git workflow: Pull latest changes, make edits, stage changes (add), commit with message, push to remote repository."
      },
      {
        "id": "git-007",
        "topic": "git",
        "difficulty": "medium",
        "question": "What is git cherry-pick?",
        "options": [
          "Selecting files to commit",
          "Applying specific commits from one branch to another",
          "Deleting unwanted commits",
          "Merging entire branches"
        ],
        "correct_answer": 2,
        "explanation": "Git cherry-pick allows you to apply specific commits from one branch to another, useful for selectively bringing changes without merging entire branches.",
        "scenario": "You need to apply a specific bug fix from a feature branch to the main branch without merging the entire feature."
      },
      {
        "id": "git-008",
        "topic": "git",
        "difficulty": "hard",
        "question": "When do you use 'git rebase' instead of 'git merge'?",
        "options": [
          "Always use rebase instead of merge",
          "To create a linear history and avoid merge commits",
          "Only for reverting changes",
          "When working with remote branches only"
        ],
        "correct_answer": 2,
        "explanation": "Use rebase to create a linear history by re-applying commits on top of another branch, avoiding merge commits. Don't rebase shared/public branches.",
        "scenario": "You want to maintain a clean, linear commit history when integrating feature branches."
      },
      {
        "id": "cicd-007",
        "topic": "cicd",
        "difficulty": "easy",
        "question": "What is meant by Continuous Integration?",
        "options": [
          "Deploying code once per month",
          "Frequently integrating code changes with automated testing",
          "Manual testing of all features",
          "Continuous monitoring only"
        ],
        "correct_answer": 2,
        "explanation": "Continuous Integration is the practice of frequently integrating code changes into a shared repository, with automated builds and tests to detect issues early."
      },
      {
        "id": "cicd-008",
        "topic": "cicd",
        "difficulty": "medium",
        "question": "What is blue-green deployment?",
        "options": [
          "Using blue and green colors in UI",
          "Two identical environments where one serves traffic while the other is updated",
          "Deploying to development and production simultaneously",
          "A security scanning technique"
        ],
        "correct_answer": 2,
        "explanation": "Blue-green deployment uses two identical production environments. One (blue) serves live traffic while the other (green) is updated, then traffic is switched instantly.",
        "scenario": "You need zero-downtime deployments for a critical e-commerce application."
      },
      {
        "id": "cicd-009",
        "topic": "cicd",
        "difficulty": "medium",
        "question": "What is a canary deployment?",
        "options": [
          "Deploying to a bird-themed environment",
          "Gradually rolling out changes to a small subset of users first",
          "Deploying only during specific hours",
          "Backing up data before deployment"
        ],
        "correct_answer": 2,
        "explanation": "Canary deployment gradually rolls out changes to a small percentage of users first, monitoring for issues before full rollout.",
        "scenario": "You want to test new features with real users while minimizing risk of widespread issues."
      },
      {
        "id": "ansible-001",
        "topic": "ansible",
        "difficulty": "easy",
        "question": "What is Ansible?",
        "options": [
          "A programming language",
          "An agentless automation tool for configuration management",
          "A database management system",
          "A monitoring tool"
        ],
        "correct_answer": 2,
        "explanation": "Ansible is an agentless automation tool used for configuration management, application deployment, and orchestration using SSH connections."
      },
      {
        "id": "ansible-002",
        "topic": "ansible",
        "difficulty": "medium",
        "question": "What protocol does Ansible use for communicating with client systems?",
        "options": [
          "HTTP",
          "SSH",
          "FTP",
          "SMTP"
        ],
        "correct_answer": 2,
        "explanation": "Ansible uses SSH (Secure Shell) to communicate with client systems, making it agentless as no special software needs to be installed on managed nodes."
      },
      {
        "id": "ansible-003",
        "topic": "ansible",
        "difficulty": "medium",
        "question": "What is an inventory file in Ansible?",
        "options": [
          "A list of tasks to execute",
          "A file that defines hosts and groups that Ansible manages",
          "A backup of system configurations",
          "A log of executed commands"
        ],
        "correct_answer": 2,
        "explanation": "An inventory file defines the hosts and groups of hosts that Ansible can manage, specifying connection details and grouping for organized management.",
        "scenario": "You need to manage multiple servers and want to organize them into logical groups for different environments."
      },
      {
        "id": "azure-001",
        "topic": "azure",
        "difficulty": "easy",
        "question": "What is Azure?",
        "options": [
          "A programming language",
          "Microsoft's cloud computing platform",
          "A database system",
          "An operating system"
        ],
        "correct_answer": 2,
        "explanation": "Azure is Microsoft's cloud computing platform that provides various cloud services including computing, analytics, storage, and networking."
      },
      {
        "id": "azure-002",
        "topic": "azure",
        "difficulty": "medium",
        "question": "What are ARM templates in Azure?",
        "options": [
          "CPU architecture specifications",
          "JSON templates for defining and deploying Azure resources",
          "Security access controls",
          "Network configuration files"
        ],
        "correct_answer": 2,
        "explanation": "ARM (Azure Resource Manager) templates are JSON files that define the infrastructure and configuration for Azure resources, enabling Infrastructure as Code.",
        "scenario": "You want to deploy consistent Azure environments across development, staging, and production."
      },
      {
        "id": "azure-003",
        "topic": "azure",
        "difficulty": "medium",
        "question": "How is Azure App Service different from Azure Functions?",
        "options": [
          "No difference, they're the same service",
          "App Service is for full applications, Functions is for serverless event-driven code",
          "App Service is cheaper than Functions",
          "Functions only work with databases"
        ],
        "correct_answer": 2,
        "explanation": "Azure App Service hosts complete web applications, while Azure Functions is a serverless compute service for event-driven, short-running code execution.",
        "scenario": "You need to choose between hosting a full web application vs implementing specific business logic triggered by events."
      }
    ]
}